{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8, Part 2: Pandas\n",
    "\n",
    "This notebook is based on the official `pandas` [documentation](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html).  Unless otherwise credited, quoted text comes from this document.\n",
    "\n",
    "`Pandas` is a Python package that we will use to manage labeled data.\n",
    "\n",
    "***Note*** You might want to consider a quick review of the previous tutorial (Pandas Part1) before continuing.\n",
    "\n",
    "## Instructions\n",
    "This tutorial provides step-by-step training divided into numbered sections. The sections often contain embeded exectable code for demonstration.  This tutorial is accompanied by a practice notebook with the exact same name but with a `-Practice` suffix.  Throughout this tutorial sections labeled \"Practice Task\" are interspersed and indicated with the icon: ![Task](http://icons.iconarchive.com/icons/sbstnblnd/plateau/16/Apps-gnome-info-icon.png).\n",
    "  You should follow the instructions provided in these sections by performing them in the practice notebook.  When the tutorial is completed you can turn in the final practice notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "First, we need to import the pandas library (and Numpy library too).  All packages are imported at the top of the notebook. Execute the code in the following cell to get started with this notebook (type Ctrl+Enter in the cell below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will use some of the data objects created in the previous Pandas Part 1 tutorial. Specifically these objects.\n",
    "\n",
    "Recreate:\n",
    "+ `df`:  a generic data frame containing two columns named \"alpha\" and \"beta\".\n",
    "+ `iris_df`:  a data frame containing the imported iris dataset.\n",
    "\n",
    "First, let's create the `df` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {'alpha': [0, 1, 2, 3, 4],\n",
    "     'beta': ['a', 'b', 'c', 'd', 'e']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read in the iris data.  It should be in a `data` directory inside the same directory as this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('data/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, execute the following to view the `df` data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's review the `iris_df` data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1a: Setup\n",
    "\n",
    "<span style=\"float:right; margin-left:10px; clear:both;\">![Task](http://icons.iconarchive.com/icons/sbstnblnd/plateau/96/Apps-gnome-info-icon.png)\n",
    "</span>\n",
    "\n",
    "Prepare the practice notebook:\n",
    "- import pandas\n",
    "- re-create the `df` data frame\n",
    "- re-create the `iris_df` data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Setting values in a DataFrame object\n",
    "\n",
    "We often want to change or assign data at specific rows, columns, indexes or slices.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Inserting a New Column\n",
    "\n",
    "A new column can be added by using a new label as an index to the data frame, and assigning values to it. Let's add a new column to the `df` object. The column will be named `gamma` and consist of a Series of 5 numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_series = pd.Series([4, 3, 2, 1, 0])\n",
    "df['gamma'] = new_series\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can use a numpy array instead of a Seris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gamma'] = np.array([4, 3, 2, 1, 0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Numpy, the array of values provided must have the same number of values as there are rows in the data frame. Observe the effect if the list is too short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gamma'] = np.array([4, 3, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Howevever with a Series NaN's are used to indicate missing values.  Let's add a new \"epsilon\" column that is **shorter** than the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['epsilon'] = pd.Series([1, 2, 3])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the effect if a `pd.Series` object is **longer** than the other columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['theta'] = pd.Series([0, 1, 3, 4, 5, 6])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the Series that were beyond the length of the data frame were excluded.\n",
    "\n",
    "It is also possible to overwrite existing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gamma'] = pd.Series([1, 1, 1, 1, 1])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to adding new columns, adding a single new rows is not typically done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why?**\n",
    "\n",
    "`pd.Series` objects have indexes! When we add them, Pandas adds and aligns their values by their indexes. By default these are integer indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2a: Inserting Columns\n",
    "\n",
    "<span style=\"float:right; margin-left:10px; clear:both;\">![Task](http://icons.iconarchive.com/icons/sbstnblnd/plateau/96/Apps-gnome-info-icon.png)\n",
    "</span>\n",
    "\n",
    "Create a copy of `df`, and modify it:\n",
    "\n",
    "+ add a new column to the `\n",
    "+ select and set values using a boolean operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Missing Data\n",
    "\n",
    "As shown in the previous section, missing values are represented as 'NaN' in the table display. You can test for missing values or add missing values using the Numpy `np.nan` value: \n",
    "\n",
    "> pandas primarily uses the value `np.nan` to represent missing data. It is by default not included in computations. See the [Missing Data section](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#missing-data).\n",
    "\n",
    "You can read more about the special types of float objects that are `np.nan` and `np.inf` [here](https://docs.scipy.org/doc/numpy-1.13.0/user/misc.html).\n",
    "\n",
    "The choice of what to do with missing data is specific to the analytic you are performing. Two common approaches are to drop rows (or columns) with missing data, or to impute (fill) the empty cells.  To explore this, first build a dataframe with missing values. An easy way to do this is to assign a series that is shorter than the rest to an existing dataframe.  First let's look again at the `df` data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduced missing values when we added the \"epsilon\" column.  Let's remove all rows with any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: By default, `dropna()` (and fair number of other functions) do not modify the data frame 'in place', rather they return a modified copy.  We did not store this modified data frame in another variable in the code above, so Python prints it.  Thus, observe that if we print the `df` object it remains intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to have the data frame change \"in place\", then you then have two choices:\n",
    "+ Use the `dropna` function's `inplace=True` argument.\n",
    "+ Assign the result, using the same name.\n",
    "    `df = df.dropna()`\n",
    "\n",
    "In some cases, setting missing values to some other value (such as 0) may be appropriate.  This can be accomplished using the `df.fillna()` function and passing in the desired value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3a: Missing Data\n",
    "\n",
    "<span style=\"float:right; margin-left:10px; clear:both;\">![Task](http://icons.iconarchive.com/icons/sbstnblnd/plateau/96/Apps-gnome-info-icon.png)\n",
    "</span>\n",
    "\n",
    "Create two new copies of the `df` dataframe:\n",
    "\n",
    "+ One with the `NaN` values replaced with a value of your choice.\n",
    "+ One with the rows with `NaN` values removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Operations\n",
    "\n",
    "Pandas comes with functions that \"operate\" or act on on rows or columns. Some of these include calculating the mean, covariance, correlation, percent change, etc.\n",
    "\n",
    "### 4.1 Mathematical Operations\n",
    "\n",
    "To explore these operations, Let's review the `iris_df` data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to demonstrate the use of operation lets caculate the mean of each column.  The `df.mean()` provides this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we can call `help()` on an existing object, or its abstract form.\n",
    "\n",
    "```python\n",
    "help(pd.DataFrame.mean)\n",
    "# Should be mostly the same as:\n",
    "help(iris_df.mean)\n",
    "```\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.DataFrame.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the help instructions returned by the previous line of code, how can we calculate the mean of the rows rather than the columns?  \n",
    "\n",
    "The answer is by providing an appropriate argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.mean(1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4a: Operations\n",
    "\n",
    "<span style=\"float:right; margin-left:10px; clear:both;\">![Task](http://icons.iconarchive.com/icons/sbstnblnd/plateau/96/Apps-gnome-info-icon.png)\n",
    "</span>\n",
    "\n",
    "View the [Computational tools](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html) and [statistical methods](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#method-summary) documentation.\n",
    "Using the list of operational functions choose 5 to practice using the iris data frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Apply\n",
    "\n",
    "Another common operation is the ability to apply another function to a set of rows or columns (or a subset created from slicing). To efficiently apply functions in this way there is the `df.apply` method.\n",
    "\n",
    "Here is an exceprt from the `df.apply` documentation:\n",
    "\n",
    "```python\n",
    "apply(func, axis=0, broadcast=None, raw=False, reduce=None, result_type=None, args=(), **kwds)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the full help documetnation with by executing the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(df.apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the first argument passed to `apply` is the function that should be \"applied\" to the data frame. As an exmaple, we can proivde the `print` function to print each of the columns separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `apply` to find the the data type of each column using the the built-in function `type`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `apply` to calcualte the sum of each columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: by default `apply` performs the supplied function across columns.  To apply the function across rows, use the `axis` argument (see the help for `apply` above).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4b:  Apply\n",
    "\n",
    "<span style=\"float:right; margin-left:10px; clear:both;\">![Task](http://icons.iconarchive.com/icons/sbstnblnd/plateau/96/Apps-gnome-info-icon.png)\n",
    "</span>\n",
    "\n",
    "Practice using `apply` on either the `df` or `iris_df` data frames using any two functions of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Occurances\n",
    "\n",
    "You can calculate the number of occurances for a set of values in a `pd.Series` object using the `value_counts` function.  These counts are similar to what you would see in a Histogram.  As an example, let's create a new series containing 10 random integers between 1 and 7. Then we will use `value_counts` to observe how many occurances there are of each integer.  First let's create the series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.random.randint(0, 7, size=10))\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use `value_counts` to create our \"histogram\" or occurances of each integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4c.  Occurances\n",
    "\n",
    "<span style=\"float:right; margin-left:10px; clear:both;\">![Task](http://icons.iconarchive.com/icons/sbstnblnd/plateau/96/Apps-gnome-info-icon.png)\n",
    "</span>\n",
    "\n",
    "Ientify the number of occurances for each species (virginica, versicolor, setosa) in the `iris_df` object.  *Hint*: the `value_counts` function only works on a `pd.Series` object, not on the full data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. String Methods\n",
    "\n",
    "Many times data sets will have columns with string values that must be modified in some way, such as being split from one column to two, or vice versa. \n",
    "\n",
    "To demonstrate string methods, lets adjust the `df` object to add some strings.  As a reminder lets look at the df object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder lets find the dimensions of this data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate let's create some lists of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['alpha', 'beta', 'gamma', 'delta']\n",
    "more_labels = ['foxtrot', 'whiskey', 'omega','epsilon','startrek']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll demonstrate use of the Numpy `np.random.choice` function to create two new columns for the `df` dataframe using randomized values from the two lists of strings we just created.  Take some time to make sure the following code is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iota'] = np.random.choice(labels, df.shape[0])\n",
    "df['kappa'] = np.random.choice(more_labels, df.shape[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder:** *Creating new columns with numpy arrays only works if they are of the same length as the existing dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Combining Strings\n",
    "\n",
    "Combining the strings from two different columns is easy!  For example, we can create a new column named `lambda` which contains the values of the `iota` and `kappa` columns by simply using the `+` operator.  We can include an underscore to separate the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lambda'] = df['iota'] + '_' + df['kappa']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Splitting Columns\n",
    "\n",
    "Before we split a column of strings into two different columns we must remember that each column in a data frame is a `pd.Series` object.  The `pd.Series` object has an attribute named `str`.  The `str` attribute is itself an object of type  `pandas.core.strings.StringMethods` and this object provides us with a variety of functions for working with the strings in the series... including a function named `split`. \n",
    "\n",
    "First, examine the `str` attribute of the `lambda` column of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lambda'].str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that it is an object.  We can therefore call its `split` function.  This function will iterate through each of the strings in the series and split them using the specified delimiter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lambda'].str.split('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that each row of the `lambda` column was split and a new `pd.Series` was returned with each element a list of two strings. We now have the two strings we need for the two new columns!  \n",
    "\n",
    "However, remember that the value returned from the `split` function is also a `pd.Series` object.  We need two new `pd.Series` objects (one for each new column) not one. So, we need to separate the list at each position into two separate `pd.Series` objects. We can do this by using the `str` attribute again.  To seprate the two elements into two new `pd.Series` objects we can simply assign the `str` attribute to two new variables. The values will automatically unpack: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1, split_2 = df['lambda'].str.split('_', 1).str\n",
    "split_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the `split_1` variable is now a `pd.Seires` object that contains only the first values from each element. The `split_2` will be the second set of strings.  We can simplify all of this into a single line of code that automatically adds two new columns to the `df` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split_1'], df['split_2'] = df['lambda'].str.split('_', 1).str\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5a: String Methods\n",
    "\n",
    "<span style=\"float:right; margin-left:10px; clear:both;\">![Task](http://icons.iconarchive.com/icons/sbstnblnd/plateau/96/Apps-gnome-info-icon.png)\n",
    "</span>\n",
    "\n",
    "Demonstrate the spliting and combination of columns.\n",
    "\n",
    "+ Create your list of words, as shown above.\n",
    "+ Combine two or more columns into one. *You can use string combinations, or do math on numeric columns.*\n",
    "+ Split a column into two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Combining DataFrames\n",
    "\n",
    "Often it is useful to combine two or more dataframes wither by merging, joining, contenating or grouping.  We will explore each of these operations. \n",
    "\n",
    "***Note***: For a much more exahustive description of merging, joining and contatenation, and helpful diagrams, see the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html).\n",
    "\n",
    "To demonstrate concatenation, lets use the iris data frame.  But first let's break it up into separate species-specific data frames, i.e. one dataframe per species.  We can use Fancy Indexing from the Pandas Part 1 tutorial to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa_df = iris_df[iris_df['species'] == 'setosa']\n",
    "versicolor_df = iris_df[iris_df['species'] == 'versicolor']\n",
    "virginica_df = iris_df[iris_df['species'] == 'virginica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's breifly peek at each dataframe to make sure we split them correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versicolor_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versicolor_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virginica_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virginica_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that each data frame is exactly 50 rows long and each one contains only the species specific data. Now, for the rest of this section, let's assume that we imported these iris species data as separate data frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Merging\n",
    "\n",
    "Suppose now that we imported the species-specific iris data frames and we now want to merge them.  We can do this using the `pd.concat` function.  By default, the `pd.concat` function tries to merge data frames by combining the rows of each data frame one after the other.  This image from the Pandas documentation demonstrates the process:\n",
    "\n",
    "![rows concat](https://pandas.pydata.org/pandas-docs/stable/_images/merging_concat_basic.png)\n",
    "\n",
    "The first argument that the `pd.concat` function accepts is a list of dataframes that should be merged. Therefore, we must first create a list of the individual data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = [setosa_df, versicolor_df, virginica_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call `pd.concat` to merge the three data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_iris_df = pd.concat(pieces)\n",
    "new_iris_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_iris_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the `new_iris_df` data frame is now back to 150 rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_iris_df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***, the code above works because all the columns match.  What if the columns did not match To explore this consider the following data frames from the Pandas documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'B': ['B2', 'B3', 'B6', 'B7'],\n",
    "                    'D': ['D2', 'D3', 'D6', 'D7'],\n",
    "                    'F': ['F2', 'F3', 'F6', 'F7']},\n",
    "                    index=[2, 3, 6, 7])\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                    'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "                    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "                   index=[0, 1, 2, 3])\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to merge these two data frames?  Unlike the previous example we want to merge by concatnating columns rathar than rows.  To concatenate by colmns we can pass the `axis` argument. A value of `1` tells the funtion to concatenate by columns:\n",
    "\n",
    "```python\n",
    "  pd.concat([df1, df4], axis=1)\n",
    "```\n",
    "\n",
    "However, notice that each data frame has some columns in common, and some of the row indexes are in common, but both have some columns and rows that are not shared. How will `pd.concat` handle this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df4], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the columns of the two tables are set side-by-side, even though some of the columns have the same name, and rows with the same index between the two data frames are merged.  Any columns that are missing in one data frame will have missing values added in.\n",
    "\n",
    "If we want to only include rows that have no missing values we can use the argument `join=\"inner\"` to automatically remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df4], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Append\n",
    "\n",
    "Adding rows to a dataframe is simple enough using the `append` function.  To demonstrate this, lets create a 4x6 data frame containing a collection of random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_df = pd.DataFrame(np.random.random((4, 6)))\n",
    "rand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we want to append a new row to that data frame.  Let's create a new `pd.Series` object containing the same number of random numbers as there are columns in the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_row = pd.Series(np.random.random(6) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we an call the `append` function.  Unlike the `pd.concat` function, the `append` function belongs to the `pd.DataFrame` object.  The `append` function takes the new series as its first agument. We alsO pass the `ignore_index` argument set to `True`. This forces the append to renumber the row indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_df.append(rand_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note*** the `append` function does not alter 'in place' the `rand_df` data frame. Instead it returns a new data frame with the row appended.\n",
    "\n",
    "There are more complex ways in which rows can be appended to data frames. Remember to view the [online documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html#pandas.DataFrame.append) in the future if your use cases are different from those shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6a: Concat and append data frames\n",
    "\n",
    "<span style=\"float:right; margin-left:10px; clear:both;\">![Task](http://icons.iconarchive.com/icons/sbstnblnd/plateau/96/Apps-gnome-info-icon.png)\n",
    "</span>\n",
    "\n",
    "Create two new dataframes and use them to demonstrate use of the following functions:\n",
    "\n",
    "+ `pd.concat()`\n",
    "+ `df.append()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Grouping\n",
    "\n",
    "Grouping using the `groupby` function of a `pd.DataFrame` object is a powerful sorting and subsetting tool of Pandas. The grouping performs three operations:  \n",
    "\n",
    "- Splitting the data frame\n",
    "- Applying a function\n",
    "- Combining the results\n",
    "\n",
    "\n",
    "This is best demonstrated with an example. Lets remind ourselves of the iris data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to know the mean width and length of each tissue (sepal and petal) for each species. We can create a new data frame that contains this information by \"grouping\".  Because we want to \"collapse\" our species data into a mean for each column, we want to group using the `species` column. If we run the `groupby` function using the species column we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.groupby('species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `groupBy` function returns a new object named `pandas.core.groupby.groupby.DataFrameGroupBy` or a `DataFrameGroupBy` object for short.  Lets re-run that command and save that object in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = iris_df.groupby('species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What exactly is a `DataFrameGroupBy` object?  Lets dig in to find out what is happening here. Fortunately, this object allows us to iterate over its \"groups\".  Let's see what they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in groups:   \n",
    "  print(type(group))\n",
    "  print(group)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the groups are tubles.  Remember a tuple is like a list, but unlike a list,once it is created you can not change it. Tuples are represented using parentheses, `(` and `)`, rather than square brakets.  The first element of the tuple is the key. In the example printed above the key is `setosa`.  The second element of the tuple are the rows that belong to that group.   We learn from this, that the `DataFrameGroupBy`object is a list of tuples, where each tuple contains the rows of hte data frame that belong to the group.\n",
    "\n",
    "Now that we have our groups we can apply a function and retreive a new data frame.  To get the mean we simply call `mean` on the groups object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to have `groupby()` use more than one column to group. To demonstrate, let's suppose we took measurements of the iris sepal and petals at two different developmental stages: early and late flowering. We want to cacluate the mean for those two different periods.  In this case we would need to group by the `species` and by a new development stage column.  Let's create one by randomly assigning the development stage (of course in reality this data would be provided):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df['dev_stage'] = np.random.choice(['early', 'late'], iris_df.shape[0])\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's group by both `species` and `dev_stage` and calcualte the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = iris_df.groupby(['species', 'dev_stage'])\n",
    "groups.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6b: Grouping\n",
    "\n",
    "<span style=\"float:right; margin-left:10px; clear:both;\">![Task](http://icons.iconarchive.com/icons/sbstnblnd/plateau/96/Apps-gnome-info-icon.png)\n",
    "</span>\n",
    "\n",
    "Demonstrate a `groupby`.\n",
    "\n",
    "+ Create a new column with the label `region` in the iris data frame. This column will indicates geographic regions of the US where measurments were taken. Values should include:  'Southeast', 'Northeast', 'Midwest', 'SouthWest', 'Pacific Northwest. (These can be randomly assigned).\n",
    "+ Use `groupby` to get a new data frame of means for each species in each region.\n",
    "+ Add the `dev_stage` column using the commands shown in section 6.3\n",
    "+ Use `groupby` to get a new data frame of means for each species,in each region and each development stage.\n",
    "+ Use the `count` function (just like you used the `mean` function) to identify how many rows in the table belong to each combination of species + region + developmental stage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
