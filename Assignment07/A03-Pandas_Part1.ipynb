{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8, Part 1: Pandas\n",
    "\n",
    "This notebook is based on the official `pandas` [documentation](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html).  Unless otherwise credited, quoted text comes from this document.\n",
    "\n",
    "`Pandas` is a Python package that we will use to manage labeld data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "*Where does this tool fit in my life?*\n",
    "\n",
    "> For data scientists, working with data is typically divided into multiple stages: munging and cleaning data, analyzing / modeling it, then organizing the results of the analysis into a form suitable for plotting or tabular display. pandas is the ideal tool for all of these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Why use this package?*\n",
    "\n",
    "> pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures\n",
    "\n",
    "Pandas provides two new objects in which to store and manipulate data.\n",
    "\n",
    "Dimensions | Name |\tDescription\n",
    "-----------|-------|--------------------------------------\n",
    "1 |\tSeries | \t1D labeled homogeneously-typed array\n",
    "2 |\tDataFrame | \tGeneral 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column\n",
    "\n",
    "Typical usage of this package involves the creation of one or more `DataFrame` objects, and manipulating them with the provided methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Cheat Sheet\n",
    "\n",
    "*\"[`Package Name`] cheat sheet\"* for popular packages can often be a worthwhile internet query.\n",
    "\n",
    "In the case of pandas, there is an official cheet sheat you can find [here](http://pandas.pydata.org/Pandas_Cheat_Sheet.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10* Minutes to Pandas\n",
    "\n",
    "The official [10 Minutes to Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html#min) documentation is an excellent resource, and provides the titular guide which we will drawn from and expanded upon for this tutorial.\n",
    "\n",
    "_Note: You can access the pandas documentation from within a jupyter lab session by selecting 'help' and then selecting 'pandas' from the dropdown menu._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Getting Started\n",
    "\n",
    "First, we need to import the pandas library (and Numpy library too).  All packages are imported at the top of the notebook. Execute the code in the following cell to get started with this notebook (type Ctrl+Enter in the cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy and pandas usage are often intertwined.\n",
    "# These abbreviations are ubiquitiously used.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above imports both numpy and pandas as variables named `np` and `pd`. We can use these variables to access the functionality of both libraries respectively.  The above is what we will use for the rest of this class.\n",
    "\n",
    "You may be wondering why we didn't import pandas like this:  \n",
    "```python\n",
    "import pandas\n",
    "```\n",
    "We could, but the first is far more commonly seen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.1: Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Object Creation\n",
    "\n",
    "Here we will learn to create and use the two data structures provided by Pandas: **Series** and **DataFrames**.  For additional help, see the [Data Structure Intro section](https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html#dsintro).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The `pd.Series` Data Object\n",
    "\n",
    "In Pandas, a Series is a one dimensional array, where all the entries are of the same data type (i.e. integer, string, boolean, etc.) or **NaN**.  NaN means \"not a number\" but serves as a marker when a value is missing.  Before using a series take care to consider the correct data type.\n",
    "\n",
    "The following cell creates an new series containing an array of 5 numbers and one missing value by simply passing a list of 6 elements to the `pd.Series` function. Execute the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the series object.\n",
    "my_series = pd.Series([1, 3, 5, np.nan, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell to see what the new series object named `my_series` contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the object in the notebook.\n",
    "# This (mostly) calls the __repr__() function of a given object.\n",
    "my_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2a Create a `pd.Series` object\n",
    "Use the practice notebook to create a series of your own design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The `pd.DataFrame` Data Object\n",
    "\n",
    "In Pandas, a DataFrame is a two-dimensional array.  Just like Numpy, it has axes labeled \"rows\" and \"cols\".  However, unlike Numpy, it allows for different data types in each column!  DataFrames also provide greater flexibilty for indexing and slicing.\n",
    "\n",
    "#### 2.2.1 Create a DataFrame with a dictionary\n",
    "\n",
    "A dataframe is created by passing a Python dictionary (i.e. **dict**) to the `pd.DataFrame` function.  Remember dictionaries have key/value pairs. The dictionary is interpreted in the following way:\n",
    "\n",
    "- The keys in the dictionary argument becomes the column names\n",
    "- The value in the dictionary argument contains the column values\n",
    "- Each \"column\" must have the same number of values.\n",
    "\n",
    "The following cell contains code that creates a new DataFrame object named `df_1`.  This dataframe consists of two columns, one named \"alpha\" and the other \"beta\".  Each column has 5 elements. Notice that the columns contain different data types.  Execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with a dictionary.\n",
    "df_1 = pd.DataFrame(\n",
    "    {'alpha': [0, 1, 2, 3, 4],\n",
    "     'beta': ['a', 'b', 'c', 'd', 'e']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell to see what the new data frame object named `df_1` contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the object in the notebook.\n",
    "# This time we get a html output, as the notebook shell\n",
    "# sees a _repr_html_() function to call.\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that a DataFrame can be visualized as a data table or Excel spreadsheet. Also, an index, in the left-most column, has been automatically generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Create a DataFrame with a Numpy array\n",
    "Alternatively, we can create a DataFrame by passing in a Numpy matrix.  Remember that this is only possible if the data is all of the same type as Numpy arrays can only store one data type. The following code creates a Dataframe of 6 rows and 4 columns and fills it with random numbers generated using the Numpy `np.random.randn` function.  Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(6, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our new data frame by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that because we did not provide a dictionary, we do not have header names for each column. Rather a numeric index was automatically generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2b: Create a pd.DataFrame object from a Python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Create a DataFrame with row and column labels\n",
    "\n",
    "Rather than have a numeric index for each row of the DataFrame, we may want to give each row a human-readable label. This will help make the data more accessible (as will be seen later).  \n",
    "\n",
    "To demonstrate, lets reuse the 6x4 datafame we created in section 2.2.2.  First, we need to provide our index names.  Suppose we want our row names to be dates, perhaps corresponding to when the row data was collected. The code below uses the `pd.date_range` function to do create a list of dates. If this were real data we would most likely include the dates collected as a column in the table, but for the purposes of demonstration we'll generate a series of regularly spaces dates over a 6 day period: one for each of the 6 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('20130101', periods=6)\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also suppose that we have 4 columns and we want them named 'A', 'B', 'C', and 'D'. \n",
    "\n",
    "We can re-create the DataFrame from section 2.2.2 with these names for the columns and the dates as row indexes by using the `index` and `columns` arguments when creating the DataFram as shown in the following cell.  Execute the cell to see the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2c: Supply indexes and columns when creating a pd.DataFrame object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Loading Data\n",
    "\n",
    "Creating a DataFrame manually, as in the previous examples, would be tedious for large datasets.  Fortunately, Pandas provides a variety of functions for importing data from files.  This saves you from writing your own Python code to open a file, read in the contents and format it for inclusiong into a DataFrame. For this tutorial we will look at two of these import functions:  `pd.read_csv` and `pd.read_excel()`.\n",
    "\n",
    "For this portion of the tutorial, we will use a rather [famous set of data concerning iris flowers](https://en.wikipedia.org/wiki/Iris_flower_data_set).  You can find a copy of the file [by clicking here]( https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv).\n",
    "\n",
    "![Iris flower](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/193px-Iris_versicolor_3.jpg)\n",
    "\n",
    "To use this file, create a directory named `data` in the same directory as this notebook and save the file in that directory with the name `iris.csv`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3a: Download the iris.csv file.\n",
    "Download the iris data and take a moment to learn about this data from the Wikipedia page cited above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Importing Using `pd.read_csv`\n",
    "\n",
    "The `pd.read_csv` function can be used to read in files that are in comma-separated format or tab delimited format: two very common data formats.  This function is very flexible and has a large number of arguments. See the [`pd.read_csv` online documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) for a thorough listing of arguments. \n",
    "\n",
    "The iris data is in CSV format and is compatible with the default arguments for the `pd.read_csv` function, therefore we can easily import this data by providing the path to the file as the only argument to the function. This will automatically create a DataFrame for us! Execute the following line to import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('data/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3b: Import the iris.csv file\n",
    "In your own practice notebook, import the iris dataset.\n",
    "\n",
    "#### Task 3c: Import a tab delimited file\n",
    "Take a look at the `pd.read_csv` online documentation. Describe how you would import a tab-delimited file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Importing using `pd.read_excel()`\n",
    "It is common for data to be stored in Excel worksheets (provided the data is not too large).  You can easily export data from Excel to CSV or Tab delimited formats but Pandas provides  the `pd.read_excel` function to save you the time.  We will not practice loading data from Excel, but take a look at the [`read_excel` online documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Explore your Data\n",
    "There are a variety of functions for exploring data once you have imported it. Here we will discuss `pd.head`, `pd.tail`, `pd.sample`, and `pd.describe`.\n",
    "\n",
    "### 4.1 The `pd.head` function\n",
    "\n",
    "As with any Python variable, you can display its contents by simply typing the variable name and pressing the \"enter\" key.  Howevever, for very large data files this may not be desired.  The data used in this tutorial is not extremly large but this often not the case.\n",
    "\n",
    "To easily examine your the first set of rows in your DataFrame, use the `head()` function of the `pd.DataFrame` object. By deafult it displays the first 5 rows. Execute the following cells to view the first five rows of the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an excellent way to check if you created your `pd.DataFrame` object correctly.  You can change the number of rows returned by providing an argument. The following requests the first 10 rows of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The `pd.tail` function\n",
    "Simliar to the `pd.head` function, the `pd.tail` function lets you peak at the last 5 rows of data.  To demonstrate, execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The `pd.sample` function\n",
    "The `pd.sample` function selects a random set of rows to display. By default it only selects 1 row, we can provide the number of rows we would like (e.g. 5) by providing the number as an arugment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat execution of the function to see a different set of randomly selected rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The `pd.describe` function\n",
    "The `pd.describe` function helps demonstrate the power of Pandas over Python dictionaries and Numpy arrays.  It provides a summary of statistics for the data frame in one simple function call. These statistics include the count, mean, standard deviation, min, max and quartiles.\n",
    "\n",
    "Execute the following cell to demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4a: Use head, tail and sample with the iris data frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Accessing Data\n",
    "\n",
    "In this section we will learn to retreive values, slice the data frame, and use column and row labels (rather than numeric indexes) to find data. \n",
    "\n",
    "### 5.1 Viewing column and row labels.\n",
    "Before we delve into retreiving data, lets first look at some properties of data frames for retrieving the dataframe column and row names.  This is because we often want to work with, or check the indexes and columns of a data frame using their names.  First, to retrieve the columns names we can use the `columns` property of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary, we can get a description of the index using the `index` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that here our index is numeric and runs from 0 to 150, incremented by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5a: Display the columns and indexes of the iris data frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Get all the values\n",
    "\n",
    "Perhaps you would like to extract the contents of the DataFrame into a 2-dimensional Python list. You will lose the power of Pandas, but you should know how to do this. There are two ways to extract values: the new Pandas v0.24 way and the older way.  \n",
    "\n",
    "This following code example used to be the standard way of getting the values of a dataframe as a `numpy` array:\n",
    "\n",
    "```python\n",
    "my_array = df.values\n",
    "```\n",
    "\n",
    "This is depreciated in version 0.24 of `pandas` to instead require the use of `to_numpy()`.\n",
    "\n",
    "```python\n",
    "my_array = df.to_numpy()\n",
    "```\n",
    "\n",
    "You can verify what version of `pandas` you have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5b: Check the version of `pandas` you have, then convert a data frame to a numpy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Basic Selections\n",
    "Pandas provides a variety of row/column selectors that are similar to the way that lists and arrays are indxed in Python and Numpy. This tutorial will introduce those. However, please note:\n",
    "> While standard Python / Numpy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, '.at', '.iat', '.loc' and '.iloc'.\n",
    "\n",
    "This tutorial will also descirbe the `.at`, `.iat`, `.loc` and `.iloc` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Get a column\n",
    "The easiest way to select a single column is to use the column label! When in doubt, use the `.column` property of the DataFrame to find the list of column labels.  Using the column label will result in the column being returned as a Series object.  For example, if we want the `sepal_length` column of the iris data frame we can execute the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slength = iris_df['sepal_length']\n",
    "slength.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe, we can use the `.head` function to get the first few rows of a Series as well as a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Get rows (slicing)\n",
    "Similar to slicing of Python lists, we can do the same with Pandas DataFrames:\n",
    "\n",
    "```python\n",
    "iris_df[start:end]\n",
    "```\n",
    "\n",
    "As a reminder, indexing of a list in Python uses 0-based indexing and negative numbers index in reverse order:\n",
    "\n",
    "```\n",
    " +---+---+---+---+---+---+\n",
    " | P | y | t | h | o | n |\n",
    " +---+---+---+---+---+---+\n",
    "   0   1   2   3   4   5  \n",
    "  -6  -5  -4  -3  -2  -1\n",
    "```\n",
    "\n",
    "**Important:** Recall the recommendation to *not* using this type of indexing (slicing) in production with very large data. Instead use other provided functions (`.loc`, `iloc`, etc). However, to practice, let's get the first row of the iris dataset by slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets get the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df[0:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Selecting with `df.loc` and `df.iloc`\n",
    "\n",
    "Aside from the column name and slicing (as shown in the previous sections) Pandas provides the `df.loc` and `df.iloc` accessor.  These accessors perform better for selecting subsets of the dataframe. The `.loc` accessor is used for slicing by columns and `.iloc` is used for slicing by rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Using `.loc`\n",
    "\n",
    "The `.loc` accessor allows for the slicing of the data frame by using the labels of either rows and columns.  \n",
    "\n",
    "The syntax is as follows:\n",
    "\n",
    "```python\n",
    "df.loc[row_start:row_end, column_start:column_end]\n",
    "```\n",
    "\n",
    "Remember, because the rows in the iris data frame are indexed using integers, we can use `.loc` to get rows using the integer values.  To get the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first two rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[0:1]  # Get the first two rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that when only one row is returned that a Series is provided.  When multiple rows are returned then a new DataFrame object is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, recall that we constructed a 6x4 dataframe in section 2.3 that was filled with random values and we set the row names using datetime values. The data frame was named `df`.  As a reminder here are the first 2 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data frame the rows are indexed using strings that represent dates. We can therefore use those labels to slice the dataframe.  The following extracts the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2013-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can get the first two rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2013-01-01':'2013-01-02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 Using `.loc` to get columns\n",
    "Remember for our iris data frame the column indexes have the names as provided by the `.columns` property.\n",
    "\n",
    "```python\n",
    "iris_df.columns\n",
    "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
    "       'species'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these names to retrive values for specific columns. For example, the code below extracts rows 4 through 6 and the columns 'sepal_width' and 'petal_width':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[4:6, 'sepal_width': 'petal_width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of the colon in the code above between 'sepal_width' and 'petal_width' implies a range. Therefore every column between those two are included. This is why 'petal_length' appears in the output.  If we wanted to specifically select only the two columns and none other, we could provide the names in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[4:6, ['sepal_width', 'petal_width']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that now we only have 'sepal_width', and 'petal_width' in the resulting data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can limit the rows to only those specified in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[[4,6], ['sepal_width', 'petal_width']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5c: Make selections with `loc`\n",
    "\n",
    "+ Select a single item with `at`.\n",
    "+ Select a row slice with `loc`.\n",
    "+ Select a row and column slice with `loc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 Using `.iloc`\n",
    "\n",
    "The `.iloc` accessor allows us to slice the data frame using an integer index, regardless of what the index (or column) labels actually are.\n",
    "\n",
    "The syntax is the same as `.loc` with the exception of that integers are used instead of labels:\n",
    "\n",
    "```python\n",
    "df.iloc[row_start:row_end, column_start:column_end]\n",
    "```\n",
    "To retrieve the first row of the iris data we use the numeric index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.iloc[0]  # Get the first row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use any combination of integer indexes to get a subset of rows and columns.  The following examples retieves rows 2 to 5, and excludes the 'species' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.iloc[2:5, :-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5d: Make selections with `iloc` and `iat`.\n",
    "\n",
    "+ Select a single item with `iat`.\n",
    "+ Select a row slice with `iloc`.\n",
    "+ Select a row and column slice with `iloc`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 \"Fancy\" indexing with `.where`\n",
    "\n",
    "Pandas allows us to use boolean values to extract specific rows of data that meet certain conditions.To clarify this, first examine what occurs when we apply a condition to a data frame.  Recall the data frame we created in section 2.2.1.  As a reminder execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a condition to the data frame to find all values > 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 > 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the resulting data frame contains `True` and `False` values in place of every value that did or did not meet the condition.  We can use this new data frame to subset the original dataframe.\n",
    "\n",
    "Lets consider the iris dataset for a more realistic example.  Suppose we want to find all rows with a `sepal_length` greater than 5.8. We know from the call to `describe` that the mean is approximately 5.8. We can do so with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = iris_df['sepal_length'] > 5.8\n",
    "iris_df[condition].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could rewrite the two lines above to combine them into a single line with identical results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df[iris_df['sepal_length'] > 5.8].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5e: Boolean Indexing\n",
    "\n",
    "Create subsets using boolean indexes that:\n",
    "+ Use one boolean operator.\n",
    "+ Use two boolean operators.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
